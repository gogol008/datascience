{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- read.csv(file='/root/hackerday/06_san_francisco_salary/train.csv', header=TRUE,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test <- read.csv(file='/root/hackerday/06_san_francisco_salary/test.csv', header=TRUE,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. xgboost --boosting algorithm\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = names(train[2:ncol(train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train$label = as.numeric(train$label)\n",
    "train[var] =sapply(train[var], function(x) as.numeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train = xgb.DMatrix(data=data.matrix(train[var]),label=(train$label))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parameters to be passed in xgboost\n",
    "param <- list(objective           = \"multi:softprob\", \n",
    "              booster = \"gbtree\",\n",
    "              eta                 = 0.01, # 0.06, #0.01,0.005\n",
    "              max_depth           = 10, #changed from default of 4,6,8,10,15,20\n",
    "              subsample           = 0.7, #(.5,0.7,1)\n",
    "              colsample_bytree    = 0.7, #(.5,0.7,1)\n",
    "              min_child_weight=2,  ## 3/ Event rate - Rule of Thumb \n",
    "              num_class = 10\n",
    ")\n",
    "\n",
    "## Train the model with 2 fold cross validation\n",
    "clf <- xgb.cv(params              = param, \n",
    "              data                = all_train, \n",
    "              nrounds             = 200, #300, #280, #125, #250, # changed from 300\n",
    "              verbose             = 1,\n",
    "              #early.stop.round    = 40,\n",
    "              #watchlist           = watchlist,\n",
    "              maximize            = FALSE,\n",
    "              eval_metric=\"mlogloss\",\n",
    "              nfold=2\n",
    ")\n",
    "\n",
    "###### Use the entire training set using best parameters \n",
    "clf_best <- xgboost(params        = param, \n",
    "                    data                = all_train, \n",
    "                    nrounds             = 100, #300, #280, #125, #250, # changed from 300\n",
    "                    verbose             = 1,\n",
    "                    #early.stop.round    = 200,\n",
    "                    #watchlist           = watchlist,\n",
    "                    maximize            = FALSE,\n",
    "                    eval_metric=\"mlogloss\"\n",
    "                    #nfold=3\n",
    ")\n",
    "\n",
    "imp_var_test <- names(test[1:ncol(test)])\n",
    "test[imp_var_test] <- sapply(test[imp_var_test],function(x) as.numeric(x))\n",
    "testDataMatrix<-xgb.DMatrix(data=data.matrix((test[imp_var_test])))\n",
    "pred1 <- predict(clf_best,testDataMatrix)\n",
    "pred = matrix(pred1, nrow=1)\n",
    "pred = data.frame(t(pred))\n",
    "\n",
    "pred2 <- data.frame(ImageId=1:nrow(test),Label=pred)\n",
    "write.csv(pred2,\"xgboost_bm.csv\",row.names = F)\n",
    "    \n",
    "    \n",
    "##neural network in H20\n",
    "    \n",
    "    #install.packages(\"h2o\")\n",
    "library(h2o)\n",
    "## start a local h2o cluster\n",
    "localH2O = h2o.init(max_mem_size = '6g', # use 6GB of RAM of *GB available\n",
    "                    nthreads = -1) # use all CPUs (8 on my personal computer :3)\n",
    "\n",
    "## MNIST data as H2O\n",
    "train[,1] = as.factor(train[,1]) # convert digit labels to factor for classification\n",
    "train_h2o = as.h2o(train)\n",
    "test_h2o = as.h2o(test)\n",
    "\n",
    "\n",
    "## set timer\n",
    "s <- proc.time()\n",
    "\n",
    "## train model\n",
    "model =\n",
    "  h2o.deeplearning(x = 2:785,  # column numbers for predictors\n",
    "                   y = 1,   # column number for label\n",
    "                   training_frame = train_h2o, # data in H2O format\n",
    "                   activation = \"RectifierWithDropout\", # algorithm\n",
    "                   input_dropout_ratio = 0.2, # % of inputs dropout\n",
    "                   hidden_dropout_ratios = c(0.5,0.5), # % for nodes dropout\n",
    "                   balance_classes = TRUE, \n",
    "                   hidden = c(100,100), # two layers of 100 nodes\n",
    "                   momentum_stable = 0.99,\n",
    "                   nesterov_accelerated_gradient = T, # use it for speed\n",
    "                   epochs = 15) # no. of epochs\n",
    "\n",
    "\n",
    "## print confusion matrix\n",
    "h2o.confusionMatrix(model)\n",
    "\n",
    "## print time elapsed\n",
    "s - proc.time()\n",
    "\n",
    "## classify test set\n",
    "h2o_y_test <- h2o.predict(model, test_h2o)\n",
    "\n",
    "## convert H2O format into data frame and  save as csv\n",
    "df_y_test = as.data.frame(h2o_y_test)\n",
    "df_y_test = data.frame(ImageId = seq(1,length(df_y_test$predict)), Label = df_y_test$predict)\n",
    "write.csv(df_y_test, file = \"submission-r-h2o.csv\", row.names=F)\n",
    "\n",
    "## shut down virutal H2O cluster\n",
    "h2o.shutdown(prompt = F)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
